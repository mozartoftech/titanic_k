{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"train.csv\"\n",
    "titanic = pd.read_csv(data)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info() # indicates we have missing values in 3 columns. 891 rows and 12 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe() # summary statistics for numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = titanic.isnull().sum() # Age - 177 missing values, Cabin - 687 missing values, Embarked - 2 missing values\n",
    "missing_values[missing_values > 0] # displays columns with missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values \n",
    "\n",
    "# dropping the \"Cabin\" column\n",
    "\n",
    "titanic = titanic.drop(columns=[\"Cabin\"])\n",
    "\n",
    "# dropping rows where \"Embarked\" is missing\n",
    "\n",
    "titanic = titanic.dropna(subset=[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing Age values with the median\n",
    "\n",
    "titanic[\"Age\"].fillna(titanic[\"Age\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at survival rate by gender \n",
    "\n",
    "titanic.groupby(\"Sex\")[[\"Survived\"]].mean() # 3 out every 4 females survived, 1 out of every 5 males survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows how class and gender affected the survival rate \n",
    "\n",
    "titanic.groupby([\"Sex\", \"Pclass\"])[\"Survived\"].aggregate(\"mean\").unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pivot table which was introduced for convinence can handle the multidimensional aggregation as seen above \n",
    "\n",
    "# The sex and the classes are actually good predictors of survival \n",
    "titanic.pivot_table(\"Survived\", index=\"Sex\", columns=\"Pclass\") # makes the code a lot easier to read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilevel pivot tables:\n",
    "# Using the Age variable as a third dimension:\n",
    "\n",
    "age = pd.cut(titanic[\"Age\"], [0, 18, 80]) # bining the Age variable into 3 categories\n",
    "titanic.pivot_table(\"Survived\", [\"Sex\", age], \"Pclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare = pd.qcut(titanic[\"Fare\"], 2)\n",
    "titanic.pivot_table(\"Survived\", [\"Sex\", age, \"Embarked\"], \"Pclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(\"Embarked\")[[\"Survived\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(\"Survived\", index=\"Embarked\", columns=\"Pclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the distribution of individual features and how they relate to the target variable \n",
    "\n",
    "# Analyzing Numerical features:\n",
    "\n",
    "# Age: Age is an important factor as certain age groups (like children) may have been prioritized for survival \n",
    "\n",
    "# Plotting the Age distribution:\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(titanic[\"Age\"], bins=30, kde=True) # kde overlays a smooth curve that helps to see the overall shape\n",
    "plt.title(\"Age Distribution\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.show() # has a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot to visualize the relationship between Age and Survived: there is likely to be more correlation with lesser age and survival\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"Survived\", y=\"Age\", data=titanic, palette=\"Set2\")\n",
    "plt.title(\"Age vs Survived\")\n",
    "plt.xlabel(\"Survived\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.xticks([0, 1], [\"Did Not Survive\", \"Survived\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare: Higher prices could indicate wealthier passengers who may have had higher chances of survival\n",
    "\n",
    "# Plotting the distribution for Fare:\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(titanic[\"Fare\"], bins=30, kde=True)\n",
    "plt.title(\"Fare Distribution\")\n",
    "plt.xlabel(\"Fare\")\n",
    "plt.show() # has a right-skewed distribution since most passengers likely paid lower fares, with fewer high-paying outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for Fare and Survival: passengers with higher Fare prices has more chances of survival\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"Survived\", y=\"Fare\", data=titanic, palette=\"coolwarm\")\n",
    "plt.title(\"Fare vs Survived\")\n",
    "plt.xlabel(\"Survived\")\n",
    "plt.ylabel(\"Fare\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(titanic[\"SibSp\"], kde=True)\n",
    "plt.title(\"Siblings/Spouse distribution\")\n",
    "plt.xlabel(\"Sibsp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(titanic[\"Parch\"], kde=True)\n",
    "plt.title(\"Siblings/Spouse distribution\")\n",
    "plt.xlabel(\"Sibsp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Categorical Features: Pclass, Sex, Embarked\n",
    "\n",
    "# Pclass: Passenger's class can indicate social status, which may correlate with survival rates\n",
    "\n",
    "# Countplot for Pclass: Plots the survival rate for each class\n",
    "\n",
    "# A countplot displays the count of survivors and non-survivors within each class, \n",
    "# making it easy to see how survival varied across classes.\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=\"Pclass\", hue=\"Survived\", data=titanic)\n",
    "plt.title(\"Pclass vs Survival\")\n",
    "plt.xlabel(\"Pclass\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Survived\")\n",
    "plt.show() # a cool viz!. Indicates that the Pclass feature is a good predictor for survival tendencies.\n",
    "# socio-economic status influenced survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex: Gender may have affected survival as due to the \"women and children first\" policy\n",
    "\n",
    "# Plotting survival rates by gender:\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=\"Sex\", hue=\"Survived\", data=titanic)\n",
    "plt.title(\"Sex vs Survival\")\n",
    "plt.xlabel(\"Sex\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Survived\")\n",
    "plt.show() # the female had more survival tendencies than men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked: The port of embarkation could influence survival, as different ports had different type of passengers\n",
    "\n",
    "# Countplot for Embarkation:\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=\"Embarked\", hue=\"Survived\", data=titanic)\n",
    "plt.title(\"Embarked vs Survival\")\n",
    "plt.xlabel(\"Embarked\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This involves transforming raw data into useful features to make patterns in the data clearer for machine learning models \n",
    "New features can be created, categorical variables can be encoded and numerical values can be normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features:\n",
    "\n",
    "# Creating an Age Group feature:\n",
    "\n",
    "# define Age categories:\n",
    "\n",
    "# The labels will allow for the analysis of survival by age category \n",
    "titanic[\"AgeGroup\"] = pd.cut(titanic[\"Age\"], bins=[0, 12, 18, 60, 100], labels=[\"Child\", \"Teen\", \"Adult\", \"Senior\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Family size, derived from SibSp (siblings/spouses) and Parch (parents/children), might indicate whether traveling in\n",
    "# groups influenced survival.\n",
    "\n",
    "# Creating a Family Size feature:\n",
    "\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"] + 1 # plus 1 for the passenger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=\"FamilySize\", hue=\"Survived\", data=titanic)\n",
    "plt.title(\"Family Size vs Survival\")\n",
    "plt.xlabel(\"Family Size\")\n",
    "plt.legend(title=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traveling alone versus with a group could have impacted survival chances, so it’s useful to flag passengers who had no \n",
    "# family with them.\n",
    "\n",
    "# Creating Alone feature:\n",
    "\n",
    "titanic[\"Alone\"] = (titanic[\"FamilySize\"] == 1).astype(int) # .astype(int) converts the result to integer value (0 or 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Machine learning models require numerical data, so we need to convert categorical features into numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Sex feature:\n",
    "\n",
    "titanic[\"Sex\"] = titanic[\"Sex\"].map({\"male\": 0, \"female\": 1}) # the will help the model to recognize gender differences numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding AgeGroup and Embarked:\n",
    "\n",
    "# AgeGroup and Embarked are multi-class categorical variables, so we’ll use one-hot encoding to convert each category \n",
    "# into its own binary column.\n",
    "\n",
    "titanic = pd.get_dummies(titanic, columns=[\"AgeGroup\", \"Embarked\"], drop_first=True, dtype=int)\n",
    "# pd.get_dummies converts each category into a new column\n",
    "# drop_first=True removes one category to avoid multicolinearity (a redundancy issue where categories are perfectly correlated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Numerical Features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "In many Machine learning models, scaling ensures that all features contribute equally by putting them on the same scale.\n",
    "This is especially important if we use models that are sensitive to feature magnitudes, like logistic regression or neural\n",
    "networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Fare:\n",
    "\n",
    "# Fare has a wide range of values. Scaling it helps to bring it to a comparable range with other features.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initializing the scaler:\n",
    "scaler = StandardScaler() # normalizes the data to have a mean of 0 and std of 1, which helps the model to interpret \n",
    "# values consistently \n",
    "\n",
    "# Scaling the Fare feature:\n",
    "titanic[\"Fare\"] = scaler.fit_transform(titanic[[\"Fare\"]]) # fit_transform learns and applys the scaling directly to Fare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check for missing values:\n",
    "\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Type Refinement:\n",
    "\n",
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addressing Class Imbalance:\n",
    "\n",
    "# Visualizing the Class Distribution:\n",
    "\n",
    "sns.countplot(x=\"Survived\", data=titanic, palette=\"viridis\")\n",
    "plt.title(\"Survival Class Distribution\")\n",
    "plt.xlabel(\"Survived\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the Name feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Titles from Name: This might help to reveal useful insights about social standing, age or even gender \n",
    "\n",
    "# We can extract titles (e.g., Mr, Mrs, Miss, Master) and use them as a new categorical feature \n",
    "\n",
    "titanic[\"Title\"] = titanic[\"Name\"].str.extract(\" ([A-Za-z]+)\\.\", expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping rare titles: Reducing unique values simplifies the model’s task, especially if some titles only appear a few times in the dataset.\n",
    "\n",
    "# Simplifying rare titles:\n",
    "\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace([\"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"], \"Rare\")\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace([\"Mlle\", \"Ms\"], \"Miss\")\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace(\"Mme\", \"Mrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=\"Title\", hue=\"Survived\", data=titanic)\n",
    "plt.title(\"Title vs Survived\")\n",
    "plt.xlabel(\"Title\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Title as Categorical Feature:\n",
    "\n",
    "# One-hot encode Title feature:\n",
    "\n",
    "titanic = pd.get_dummies(titanic, columns=[\"Title\"], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the original Name and Ticket columns:\n",
    "\n",
    "titanic = titanic.drop([\"Name\", \"Ticket\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head() # we are left with numerical categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "\n",
    "X = titanic.drop(\"Survived\", axis=1)\n",
    "y = titanic[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and test sets:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "In machine learning, the training process involves selecting a model type (like logistic regression, decision trees, etc.) and then “fitting” \n",
    "it to our dataset. This allows the model to learn patterns and relationships between the input features and the target variable (Survived). \n",
    "After training, we evaluate the model's ability to generalize by testing it on unseen data (our test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training with Logistic Regrssion:\n",
    "\n",
    "#  A common choice for binary classification problems. It calculates probabilities and can work well with fewer features.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initializing the Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model to the training data:\n",
    "\n",
    "# Fitting (or training) the model allows it to learn patterns in the training data. The fit method updates the model’s parameters to minimize\n",
    "# errors in predicting the target variable (Survived).\n",
    "\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Evaluation helps us understand how well the model performs on unseen data. We'll use the test set (X_test and y_test) to check if the\n",
    "model generalizes well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions:\n",
    "\n",
    "# Generate predictions on the test set\n",
    "\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating with Accuracy Score:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculating Accuracy:\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating with Classification Report:\n",
    "\n",
    "# A classification report provides a deeper look, including precision, recall, and f1-score for each class, \n",
    "# helping us understand performance beyond accuracy.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print classification report \n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Alternative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing and Evaluating Each Model:\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Looping through each model, training it, and checking its accuracy on the test set:\n",
    "\n",
    "# Dictionary of models:\n",
    "\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42), \n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating each model:\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Train the model:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions:\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy:\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{model_name} Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set score: {:.2f}\".format(log_reg.score(X_test, y_test))) # just checking..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Hyperparameters control the behavior of a model, such as how it learns or regularizes to prevent overfitting. \n",
    "Tuning helps identify the optimal values to maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Grid Search for Hyperparmeter tuning: Grid Search is a method to exhaustively search over a set of specified hyperparameters to find the \n",
    "# best combination\n",
    "\n",
    "# C - Controls regularization strength. Lower values mean stronger regularization strength.\n",
    "# solver - Determines the optimization algorithm. Different solvers can have an impact on performance.\n",
    "# penalty - Regularization type (l1, l2, etc.), which helps control model complexity and prevent overfitting.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.02, 0.2, 2, 20, 200], \n",
    "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"],\n",
    "    \"penalty\": [\"l2\"]\n",
    "} \n",
    "\n",
    "\n",
    "# Initializing grid search:\n",
    "\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(random_state=42), param_grid=param_grid, cv=5, scoring=\"accuracy\", n_jobs=1)\n",
    "\n",
    "# Perform grid search on training data\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters and Best score\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_ * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Logistic Regression with the Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best parameters from grid search \n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with best parameters\n",
    "\n",
    "optimized_model = LogisticRegression(**best_params, random_state=42)\n",
    "optimized_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions \n",
    "\n",
    "y_pred_optimized = optimized_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "\n",
    "accuracy_optimized = accuracy_score(y_test, y_pred_optimized)\n",
    "\n",
    "print(f\"Optimized Logistic Regression Accuracy: {accuracy_optimized * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamter Tuning for Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest \n",
    "\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 100, 200],          # Number of trees\n",
    "    \"max_depth\": [None, 10, 20, 30],         # Maximum depth of trees\n",
    "    \"min_samples_split\": [2, 5, 10],         # Minimum samples to split a node\n",
    "    \"min_samples_leaf\": [1, 2, 4],           # Minimum samples at leaf node\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"] # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid_rf, cv=5, scoring=\"accuracy\", n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Grid Search:\n",
    "\n",
    "# Training the Random Forest model with different combinations of hyperparameters and evaluating each one using 5-fold cross-validation.\n",
    "\n",
    "# Fit grid search on training data\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and the best cross-validation accuracy score\n",
    "\n",
    "print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy for Random Forest:\", grid_search_rf.best_score_ * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Random Forest Model with Optimized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters from grid search \n",
    "\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "\n",
    "# Train Random Forest with best parameters \n",
    "\n",
    "optimized_rf = RandomForestClassifier(**best_params_rf, random_state=42)\n",
    "optimized_rf.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions \n",
    "\n",
    "y_pred_rf_optimized = optimized_rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "\n",
    "accuracy_rf_optimized = accuracy_score(y_test, y_pred_rf_optimized)\n",
    "print(f\"Optimized Random Forest Accuracy: {accuracy_rf_optimized * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on the Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading test data \n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = test_data.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(columns=[\"Cabin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"Age\"].fillna(test_data[\"Age\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"Fare\"].fillna(test_data[\"Fare\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering the test data\n",
    "\n",
    "test_data[\"AgeGroup\"] = pd.cut(test_data[\"Age\"], bins=[0, 12, 18, 60, 100], labels=[\"Child\", \"Teen\", \"Adult\", \"Senior\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"FamilySize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"Alone\"] = (test_data[\"FamilySize\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Variables \n",
    "\n",
    "test_data[\"Sex\"] = test_data[\"Sex\"].map({\"male\": 0, \"female\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.get_dummies(test_data, columns=[\"AgeGroup\", \"Embarked\"], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Numerical Features\n",
    "\n",
    "test_data[\"Fare\"] = scaler.fit_transform(test_data[[\"Fare\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the name feature \n",
    "\n",
    "test_data[\"Title\"] = test_data[\"Name\"].str.extract(\" ([A-Za-z]+)\\.\", expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"Title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data[\"Title\"] = test_data[\"Title\"].replace([\"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"], \"Rare\")\n",
    "test_data[\"Title\"] = test_data[\"Title\"].replace([\"Mlle\", \"Ms\"], \"Miss\")\n",
    "test_data[\"Title\"] = test_data[\"Title\"].replace(\"Mme\", \"Mrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = pd.get_dummies(test_data, columns=[\"Title\"], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop([\"Name\", \"Ticket\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = optimized_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame with PassengerId and the predicted Survived labels\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_data[\"PassengerId\"],\n",
    "    \"Survived\": test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
